{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Data_Creation_&_Cleaning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OT_h4SxIgWYW","executionInfo":{"status":"ok","timestamp":1614684608874,"user_tz":-330,"elapsed":43855,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"ae823021-4fe7-49ca-b662-8a35257ff270"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_s1MStvYKWGT"},"source":["#Modules Installation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kpxdlo5B5cKZ","executionInfo":{"status":"ok","timestamp":1614684622283,"user_tz":-330,"elapsed":9358,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"421ee444-3491-4ffb-a63d-97433cd78c59"},"source":["!pip install -q twarc\n","!pip install -q jsonlines"],"execution_count":2,"outputs":[{"output_type":"stream","text":["  Building wheel for twarc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"arSOg0PHKVFN"},"source":["#Import Modules"]},{"cell_type":"code","metadata":{"id":"KkQi21naXgaL","executionInfo":{"status":"ok","timestamp":1614684637755,"user_tz":-330,"elapsed":1862,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["import os,zipfile,glob\n","import pandas as pd\n","import twarc"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwKYV-kmKUUH"},"source":["#Total IEEE Dataset \n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyfwx1ungbDB","executionInfo":{"status":"ok","timestamp":1614684643189,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"a6c3bf37-b8ab-449a-b8af-1df15944ea7e"},"source":["path = \"/content/drive/My Drive/Tweet_Csv_File/*.csv\"\n","csv_list = glob.glob(path) # collecting all files  same path \n","print(len(csv_list))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["61\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D6ejH8LTd_jS","executionInfo":{"status":"ok","timestamp":1614684819649,"user_tz":-330,"elapsed":175725,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["data = pd.DataFrame()\n","for f in csv_list:\n","  data1 = pd.read_csv(f,header=None)#reading the csv file\n","  data = pd.concat([data,data1],ignore_index=True)#concating the two data frames\n","  data.reset_index(drop=True,inplace=True) #resetting the indexes"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6DA6f7WblnV","executionInfo":{"status":"ok","timestamp":1614684842486,"user_tz":-330,"elapsed":1857,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["data.rename({0:'tweetID',1:'sentiment_score'},axis=1,inplace=True)#renaming the indexes"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0yVWgbrcNro","executionInfo":{"status":"ok","timestamp":1614685329728,"user_tz":-330,"elapsed":481763,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["data.to_csv('/content/drive/My Drive/Tweet_Dataset/data.csv',index=False)#converting the dataframe to CSV"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tr3mFYloKrae","executionInfo":{"status":"ok","timestamp":1614685852833,"user_tz":-330,"elapsed":995546,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["for f in csv_list:\n","  df = pd.read_csv(f,header=None) #reading the CSV file\n","  df = df[0] #Only taking the Tweet Id's from the dataset\n","  base = os.path.basename(f) #returning the name of the file\n","  path = '/content/drive/My Drive/Tweets_ID/'+base\n","  df.to_csv(path,index=False) # converting the dataframe to CSV "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qb0V0u_LfOiT"},"source":["# Hydrating Tweets\n"]},{"cell_type":"code","metadata":{"id":"5WPefntNfZsZ","executionInfo":{"status":"ok","timestamp":1614686182024,"user_tz":-330,"elapsed":907,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["# Insert API Keys here { run : \"auto\"}\n","\n","# These keys are received after applying for a twitter developer account\n","\n","consumer_key = \"SgGkRsD2tfMxxHspKBnB2Dbr6\"\n","consumer_secret = \"cCUFZZypukNemxAoftirW9TkoFB7epqjxhLWpJ4mBLXIULl3fx\"\n","access_token = \"1299347729533906944-l9hbd1RLX7XKvLPwTiIodXL45GHmRA\"\n","access_token_secret = \"eGnqDTgv7YDJWU8OYaqkGdsJVYnQvu7ZLzSUS5Rj1KkAH\"\n","\n","t = twarc.Twarc(consumer_key, consumer_secret, access_token, access_token_secret) #Initializing Twarc"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3oU2PFEk46z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614686432238,"user_tz":-330,"elapsed":244665,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"09110815-1338-43ac-cc1b-b8fb418bcf64"},"source":["import jsonlines, json\r\n","\r\n","data = pd.read_csv('/content/drive/My Drive/SA/Geo_Location_Data_set/april28-june18.csv',header=None) #Loading IEEE Geodata\r\n","data = data[0] #Taking only Tweet IDs for Hydration\r\n","ids = data.values.tolist() #Getting list of tweet ids from pandas DataFrame\r\n","hydrated_tweets = [] #Creating empty list\r\n","ids_to_hydrate = set(ids) #Creating ids_to_hydrate list\r\n","\r\n","# Now, use twarc and start hydrating\r\n","for tweet in t.hydrate(ids_to_hydrate): #Initializing Hydrate Iterator with twarc\r\n","  if tweet['place'] != None: #Checking for place value\r\n","    if tweet['place']['country'] == 'India': #Checking if Country is India\r\n","      hydrated_tweets.append(tweet) #Appending Obtained tweet to hydrated_tweets list"],"execution_count":10,"outputs":[{"output_type":"stream","text":["ERROR:twarc:caught connection error ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')) on 1 try\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"12NJZr3GCE2oQhhjIZ5EaRLWSzJDeyjos"},"id":"FA6xlvWb1iff","executionInfo":{"status":"ok","timestamp":1614686872137,"user_tz":-330,"elapsed":11749,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"6088f8eb-45aa-44a0-e4bb-d67e4a606252"},"source":["hydrated_tweets"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wl0GE81YU7n1","executionInfo":{"status":"ok","timestamp":1614686939725,"user_tz":-330,"elapsed":956,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["output_filename = \"/content/drive/My Drive/SA/Geo_Location_Data_set/output.csv\" #output file path"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc8XM4MVk-72","executionInfo":{"status":"ok","timestamp":1614686943217,"user_tz":-330,"elapsed":1771,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["# Convert jsonl to csv\r\n","import csv, jsonlines\r\n","\r\n","# These are the column name that are selected to be stored in the csv\r\n","keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\r\n","          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \r\n","          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\r\n","          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \r\n","          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \r\n","          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \r\n","          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \r\n","          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\r\n","          \"quoted_status_permalink\"]\r\n","\r\n","# Writes them out (Saving output CSV file with the Indian COVID-19 Tweets)\r\n","with  open(output_filename, \"w+\") as output_file:\r\n","    d = csv.DictWriter(output_file, keyset)\r\n","    d.writeheader()\r\n","    d.writerows(hydrated_tweets)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1znR6POE3D6","executionInfo":{"status":"ok","timestamp":1614686958764,"user_tz":-330,"elapsed":2037,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["import pandas as pd\n","\n","data = pd.read_csv(output_filename)# reading the file\n","data1 = pd.read_csv('/content/drive/My Drive/SA/Geo_Location_Data_set/april28-june18.csv',header=None)\n","data1.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n","result = pd.merge(data,data1,on='id')#merging the two dataframes\n","result.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n","result.to_csv('/content/drive/My Drive/Tweet_Dataset/India_Dataset.csv',index=False)# Converting to CSV"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1h2mL5R4Pma5"},"source":["#*Sentiment Addition*"]},{"cell_type":"code","metadata":{"id":"o6BD7X7zez4i","executionInfo":{"status":"ok","timestamp":1614686986670,"user_tz":-330,"elapsed":10032,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv(\"/content/drive/My Drive/Tweet_Dataset/hydrated_corona_tweets_01\")\n","df = df[['id','retweet_count','lang','text']]#selection of Features\n","data = df[df[\"lang\"]==\"en\"]#Extracting the tweets from dataframe whose language is English"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tjyzx_qkdjL","executionInfo":{"status":"ok","timestamp":1614686986673,"user_tz":-330,"elapsed":4730,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}}},"source":["tweet_ids_file = \"/content/drive/My Drive/Tweet_Csv_File/corona_tweets_01.csv\" #Reference Original CSV Path"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwtOSUSHLFjn","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1614687869902,"user_tz":-330,"elapsed":926,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"34a660ec-9998-494f-cd2f-e2293cbdda51"},"source":["data = pd.read_csv(input_filename)# reading the file\n","data.rename({'text':'full_text'},inplace=True,axis=1)\n","dataset = data[['id','full_text','retweet_count']]\n","df = pd.read_csv(tweet_ids_file,header=None)\n","df.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n","output = pd.merge(dataset,df,on='id')#merging (inner) the two dataframes\n","output.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n","output.to_csv('/content/drive/My Drive/Tweet_Dataset/Dataset.csv',index=False)# Converting to CSV"],"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-8545686ac1af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# reading the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'retweet_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_ids_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#renaming the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_filename' is not defined"]}]},{"cell_type":"code","metadata":{"id":"il0DDPOA64Cf"},"source":["# Function to Convert the IEEE Sentiment score to Sentiment Text namely positive, negative, neutral\n","def func(x):\n","    if x < 0:\n","        return \"negative\"\n","    elif x == 0:\n","        return \"neutral\"\n","    else:\n","        return \"positive\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RzONZynCZp1"},"source":["df1 = pd.read_csv(\"/content/drive/My Drive/Tweet_Dataset/India_Dataset.csv\")# reading the file\n","y= df1.Sentiment\n","X= df1.drop('Sentiment',axis=1)\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.75,random_state=111)#Spliting the dataset \n","df2 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n","df3 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n","df3.reset_index(drop=True,inplace=True)#resetting the indexes\n","df3['Sentiment'] = df3['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n","df3.to_csv('/content/drive/My Drive/Final_Datasets/Test_India.csv',index=False)# Converting to CSV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVqGmdmTY3s5"},"source":["data = pd.read_csv('/content/drive/My Drive/Tweet_Dataset/Dataset.csv')# reading the file\n","df4=data.sample(n=58944,random_state=123)# sampling the dataset\n","df5=pd.concat([df2,df4],ignore_index=True)#Concating the two datasets \n","df5.reset_index(drop=True,inplace=True)# restting the indexes\n","df5['Sentiment'] = df5['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n","df5.to_csv(\"/content/drive/My Drive/Final_Datasets/Data.csv\",index=False)# Converting to CSV"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zf7r51jg9pSH"},"source":["# Splitting Final Data to Train and Test sets for DL Model\n"]},{"cell_type":"code","metadata":{"id":"ZhwlvG_O9vdD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608277655437,"user_tz":-330,"elapsed":2444,"user":{"displayName":"Akshay Nehete","photoUrl":"","userId":"09755540919697434832"}},"outputId":"32ea94f9-68bb-451d-882f-a20c732f64c6"},"source":["data = pd.read_csv('/content/drive/My Drive/Final_Datasets/Data.csv')# reading the file\n","y = data.Sentiment\n","X = data.drop('Sentiment',axis=1)\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=123)\n","df1 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n","df1.reset_index(drop=True,inplace=True)#resetting the indexes\n","df1.to_csv('/content/drive/My Drive/Final_Datasets/Train_Data.csv',index=False) # Saving Training/Validation data for DL Model\n","df2 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n","df2.reset_index(drop=True,inplace=True)#resetting the indexes\n","df2.to_csv('/content/drive/My Drive/Final_Datasets/Test_Data.csv',index=False)# Saving Test data for DL Model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,4,5,10,11,12,13,16,17,23,24,25,26,27,29,36,38,39) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"GBmOnd5K_xaZ"},"source":["#*Data Cleaning*"]},{"cell_type":"code","metadata":{"id":"wvGUzpgJ_3QS"},"source":["#Defining functions for Cleaning and Normalization of Data\n","import re\n","import string\n","def replace_url(string): # cleaning of URL\n","    text = re.sub(r'http\\S+', 'LINK', string)\n","    return text\n","\n","\n","def replace_email(text):#Cleaning of Email related text\n","    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n","    return \"\".join(line)\n","\n","def rep(text):#cleaning of non standard words\n","    grp = text.group(0)\n","    if len(grp) > 3:\n","        return grp[0:2]\n","    else:\n","        return grp# can change the value here on repetition\n","def unique_char(rep,sentence):\n","    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n","    return convert\n","\n","def find_dollar(text):#Finding the dollar sign in the text\n","    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n","    return \"\".join(line)\n","\n","def replace_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","    u\"\\U0001F600-\\U0001F64F\" # emoticons\n","    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n","    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n","    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n","    u\"\\U00002702-\\U000027B0\"\n","    u\"\\U000024C2-\\U0001F251\"\n","    \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'EMOJI', text) \n","\n","puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n","          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n","          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n","          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n","          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n","          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n","          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n","          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n","\n","def clean_text(text: str) -> str:\n","    text = str(text)\n","    for punct in puncts + list(string.punctuation):\n","        if punct in text:\n","            text = text.replace(punct, f'')\n","    return text\n","   \n","def replace_asterisk(text):\n","    text = re.sub(\"\\*\", 'ABUSE ', text)\n","    return text\n","\n","def remove_duplicates(text):\n","    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n","    return text\n","\n","def change(text):\n","    if(text == ''):\n","        return text\n","  #calling the subfunctions in the cleaning function\n","    text = replace_email(text)\n","    text = replace_url(text)\n","    text = unique_char(rep,text)\n","    text = replace_asterisk(text)\n","    text = remove_duplicates(text)\n","    text = clean_text(text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drCGOxm4Jfpe"},"source":["import pandas as pd\n","\n","# Loading different csv files for cleaning \n","pathname = \"/content/drive/My Drive/SA/Final_Datasets/Data.csv\"\n","# pathname = \"/content/drive/My Drive/SA/Final_Datasets/Train_India.csv\"\n","# pathname = \"/content/drive/My Drive/SA/Final_Datasets/Test_Data.csv\"\n","# pathname = \"/content/drive/My Drive/SA/Final_Datasets/Test_India.csv\"\n","Data = pd.read_csv(pathname) #reading the file\n","Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets\n","Data.to_csv(pathname) #converting to CSV"],"execution_count":null,"outputs":[]}]}