{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Roberta_CNN.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/SmartPracticeschool/SBSPS-Challenge-2700-Twitter-Sentiment-Analysis-Extraction-for-COVID-19/blob/master/Notebooks/Roberta_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"jDGK79LDhTvQ"},"source":["# TensorFlow roBERTa with CNN Head\n","In this notebook, we tokenize the data, create question answer targets, and how to build a custom question answer head for roBERTa in TensorFlow. Note that HuggingFace transformers don't have a `TFRobertaForQuestionAnswering` so we must make our own from `TFRobertaModel`. Roberta with CNN head is used for Twitter Sentiment Extraction.\n"]},{"cell_type":"code","metadata":{"id":"OvPoAeL-hyPJ","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"a0c4594a-50e8-4dfe-b4fe-debe64096b8e"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U62ie0eb5Nt6","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"fce5350a-8b3b-47fd-f1df-98f6fb941bae"},"source":["!pip install -q transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 675kB 10.1MB/s \n","\u001b[K     |████████████████████████████████| 3.8MB 12.2MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 59.0MB/s \n","\u001b[K     |████████████████████████████████| 890kB 51.4MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mdYH0uy_hTvQ"},"source":["# Load Libraries, Data, Tokenizer\n","We will use HuggingFace transformers [here][1]\n","\n","[1]: https://huggingface.co/transformers/"]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"fHN3YtjLhTvR","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"839826a9-5ebf-4de0-c540-1aca786c3e5f"},"source":["import pandas as pd, numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import StratifiedKFold\n","from transformers import *\n","import tokenizers\n","import math\n","print('TF version',tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TF version 2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sx1BbxJ1hTvZ","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"c5296e13-53d4-4eef-9da2-00fdef8638e9"},"source":["MAX_LEN = 310\n","PATH = '/content/gdrive/My Drive/SA/Tf-Roberta/'\n","tokenizer = tokenizers.ByteLevelBPETokenizer(\n","    vocab_file=PATH+'vocab-roberta-base.json', \n","    merges_file=PATH+'merges-roberta-base.txt', \n","    lowercase=True,\n","    add_prefix_space=True\n",")\n","EPOCHS = 3 # originally 3\n","BATCH_SIZE = 32 # originally 32\n","PAD_ID = 1\n","SEED = 88888\n","LABEL_SMOOTHING = 0.1\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n","train = pd.read_csv('/content/gdrive/My Drive/SA/tweet-sentiment-extraction/train.csv').fillna('')\n","train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cb774db0d1</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>549e992a42</td>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>Sooo SAD</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>088c60f138</td>\n","      <td>my boss is bullying me...</td>\n","      <td>bullying me</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9642c003ef</td>\n","      <td>what interview! leave me alone</td>\n","      <td>leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>358bd9e861</td>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>Sons of ****,</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID  ... sentiment\n","0  cb774db0d1  ...   neutral\n","1  549e992a42  ...  negative\n","2  088c60f138  ...  negative\n","3  9642c003ef  ...  negative\n","4  358bd9e861  ...  negative\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"o_OmPlMFhTvg"},"source":["# Training Data\n","We will now convert the training data into arrays that roBERTa understands.\n","\n","The tokenization logic below is inspired by Abhishek's PyTorch notebook [here][1].\n","\n","[1]: https://www.kaggle.com/abhishek/roberta-inference-5-folds"]},{"cell_type":"code","metadata":{"id":"gip6btWuhTvh"},"source":["ct = train.shape[0]\n","input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n","attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n","token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n","start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","\n","for k in range(train.shape[0]):\n","    \n","    # FIND OVERLAP\n","    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n","    text2 = \" \".join(train.loc[k,'selected_text'].split())\n","    idx = text1.find(text2)\n","    chars = np.zeros((len(text1)))\n","    chars[idx:idx+len(text2)]=1\n","    if text1[idx-1]==' ': chars[idx-1] = 1 \n","    enc = tokenizer.encode(text1) \n","        \n","    # ID_OFFSETS\n","    offsets = []; idx=0\n","    for t in enc.ids:\n","        w = tokenizer.decode([t])\n","        offsets.append((idx,idx+len(w)))\n","        idx += len(w)\n","    \n","    # START END TOKENS\n","    toks = []\n","    for i,(a,b) in enumerate(offsets):\n","        sm = np.sum(chars[a:b])\n","        if sm>0: toks.append(i) \n","        \n","    s_tok = sentiment_id[train.loc[k,'sentiment']]\n","    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n","    attention_mask[k,:len(enc.ids)+3] = 1\n","    if len(toks)>0:\n","        start_tokens[k,toks[0]+2] = 1\n","        end_tokens[k,toks[-1]+2] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXN3HYqfhTv7"},"source":["# Build roBERTa Model\n","We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into `bert_model` and we use BERT's first output, i.e. `x[0]` below. These are embeddings of all input tokens and have shape `(batch_size, MAX_LEN, 768)`. Next we apply `tf.keras.layers.Conv1D(filters=1, kernel_size=1)` and transform the embeddings into shape `(batch_size, MAX_LEN, 1)`. We then flatten this and apply `softmax`, so our final output from `x1` has shape `(batch_size, MAX_LEN)`. These are one hot encodings of the start tokens indicies (for `selected_text`). And `x2` are the end tokens indicies.\n"]},{"cell_type":"code","metadata":{"id":"qfM6x3cihTv8"},"source":["import pickle\n","\n","def save_weights(model, dst_fn):\n","    weights = model.get_weights()\n","    with open(dst_fn, 'wb') as f:\n","        pickle.dump(weights, f)\n","\n","\n","def load_weights(model, weight_fn):\n","    with open(weight_fn, 'rb') as f:\n","        weights = pickle.load(f)\n","    model.set_weights(weights)\n","    return model\n","\n","def loss_fn(y_true, y_pred):\n","    # adjust the targets for sequence bucketing\n","    ll = tf.shape(y_pred)[1]\n","    y_true = y_true[:, :ll]\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n","        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n","    loss = tf.reduce_mean(loss)\n","    return loss\n","\n","\n","def build_model():\n","    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n","\n","    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n","    max_len = tf.reduce_max(lens)\n","    ids_ = ids[:, :max_len]\n","    att_ = att[:, :max_len]\n","    tok_ = tok[:, :max_len]\n","\n","    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n","    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n","    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n","    \n","    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n","    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n","    x1 = tf.keras.layers.LeakyReLU()(x1)\n","    x1 = tf.keras.layers.Dense(1)(x1)\n","    x1 = tf.keras.layers.Flatten()(x1)\n","    x1 = tf.keras.layers.Activation('softmax')(x1)\n","    \n","    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n","    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n","    x2 = tf.keras.layers.LeakyReLU()(x2)\n","    x2 = tf.keras.layers.Dense(1)(x2)\n","    x2 = tf.keras.layers.Flatten()(x2)\n","    x2 = tf.keras.layers.Activation('softmax')(x2)\n","\n","    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n","    model.compile(loss=loss_fn, optimizer=optimizer)\n","    \n","    # this is required as `model.predict` needs a fixed size!\n","    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n","    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n","    \n","    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n","    return model, padded_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdvxwFQLhTwB"},"source":["# Metric"]},{"cell_type":"code","metadata":{"id":"YF9DYtLKhTwC"},"source":["def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    if (len(a)==0) & (len(b)==0): return 0.5\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EUZq0dShTwL"},"source":["# Train roBERTa Model\n","We train with 5 Stratified KFolds (based on sentiment stratification). Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction."]},{"cell_type":"code","metadata":{"id":"4_KMICnShTwO"},"source":["jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n","oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n","oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n","preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","\n","skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED) #originally 5 splits\n","for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n","\n","    print('#'*25)\n","    print('### FOLD %i'%(fold+1))\n","    print('#'*25)\n","    \n","    K.clear_session()\n","    model, padded_model = build_model()\n","        \n","    #sv = tf.keras.callbacks.ModelCheckpoint(\n","    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n","    #    save_weights_only=True, mode='auto', save_freq='epoch')\n","    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n","    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n","    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n","    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n","    # sort the validation data\n","    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n","    inpV = [arr[shuffleV] for arr in inpV]\n","    targetV = [arr[shuffleV] for arr in targetV]\n","    path = '/content/gdrive/My Drive/SA/R_CNN_weights/'\n","    weight_fn = path+'%s-roberta-%i.h5'%(VER,fold)\n","    for epoch in range(1, EPOCHS + 1):\n","        # sort and shuffle: We add random numbers to not have the same order in each epoch\n","        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n","        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n","        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n","        batch_inds = np.random.permutation(num_batches)\n","        shuffleT_ = []\n","        for batch_ind in batch_inds:\n","            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n","        shuffleT = np.concatenate(shuffleT_)\n","        # reorder the input data\n","        inpT = [arr[shuffleT] for arr in inpT]\n","        targetT = [arr[shuffleT] for arr in targetT]\n","        model.fit(inpT, targetT, \n","            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n","            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n","        save_weights(model, weight_fn)\n","\n","    print('Loading model...')\n","    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n","    load_weights(model, weight_fn)\n","\n","    print('Predicting OOF...')\n","    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n","    \n","    print('Predicting Test...')\n","    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n","    preds_start += preds[0]/skf.n_splits\n","    preds_end += preds[1]/skf.n_splits\n","    \n","    # DISPLAY FOLD JACCARD\n","    all = []\n","    for k in idxV:\n","        a = np.argmax(oof_start[k,])\n","        b = np.argmax(oof_end[k,])\n","        if a>b: \n","            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n","        else:\n","            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n","            enc = tokenizer.encode(text1)\n","            st = tokenizer.decode(enc.ids[a-2:b-1])\n","        all.append(jaccard(st,train.loc[k,'selected_text']))\n","    jac.append(np.mean(all))\n","    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4_I0_0ChTwa","outputId":"764c2a52-33c6-4c9c-8ee4-365968185d6b"},"source":["print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">>>> OVERALL 5Fold CV Jaccard = 0.7071619899295075\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQVElu9YhTwi","outputId":"1625f603-5ebf-4f0a-85ec-11698475a157"},"source":["print(jac) # Jaccard CVs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.7127858507970375, 0.7128799996992123, 0.7071942197258467, 0.701203202135898, 0.7017466772895431]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xPdAPZjLhTvw"},"source":["# Test Data\n","We must tokenize the test data exactly the same as we tokenize the training data"]},{"cell_type":"code","metadata":{"id":"QB3UTCSl4WZb","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"74c535fc-f957-42b0-dabb-c25f5183ca18"},"source":["import pandas as pd, numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import StratifiedKFold\n","from transformers import *\n","import tokenizers\n","import math\n","print('TF version',tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TF version 2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"utDnjlSp4R_v"},"source":["MAX_LEN = 310\n","PAD_ID = 1\n","num_splits = 5\n","SEED = 88888\n","\n","PATH = '/content/gdrive/My Drive/SA/Tf-Roberta/'\n","tokenizer = tokenizers.ByteLevelBPETokenizer(\n","    vocab_file=PATH+'vocab-roberta-base.json', \n","    merges_file=PATH+'merges-roberta-base.txt', \n","    lowercase=True,\n","    add_prefix_space=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6x_KDXFhTvx"},"source":["#test = pd.read_csv('/content/gdrive/My Drive/SA/tweet-sentiment-extraction/test.csv').fillna('')\n","test = pd.read_csv('/content/gdrive/My Drive/SA/Real-time-Data/tweets.csv').fillna('')\n","\n","ct = test.shape[0]\n","input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n","attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n","token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n","sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n","\n","for k in range(test.shape[0]):\n","        \n","    # INPUT_IDS\n","    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n","    enc = tokenizer.encode(text1)                \n","    s_tok = sentiment_id[test.loc[k,'sentiment']]\n","    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n","    attention_mask_t[k,:len(enc.ids)+3] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqQGW2f06ZS3"},"source":["import pickle\n","\n","def save_weights(model, dst_fn):\n","    weights = model.get_weights()\n","    with open(dst_fn, 'wb') as f:\n","        pickle.dump(weights, f)\n","\n","\n","def load_weights(model, weight_fn):\n","    with open(weight_fn, 'rb') as f:\n","        weights = pickle.load(f)\n","    model.set_weights(weights)\n","    return model\n","\n","def loss_fn(y_true, y_pred):\n","    # adjust the targets for sequence bucketing\n","    ll = tf.shape(y_pred)[1]\n","    y_true = y_true[:, :ll]\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n","        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n","    loss = tf.reduce_mean(loss)\n","    return loss\n","\n","\n","def build_model():\n","    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n","    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n","\n","    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n","    max_len = tf.reduce_max(lens)\n","    ids_ = ids[:, :max_len]\n","    att_ = att[:, :max_len]\n","    tok_ = tok[:, :max_len]\n","\n","    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n","    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n","    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n","    \n","    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n","    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n","    x1 = tf.keras.layers.LeakyReLU()(x1)\n","    x1 = tf.keras.layers.Dense(1)(x1)\n","    x1 = tf.keras.layers.Flatten()(x1)\n","    x1 = tf.keras.layers.Activation('softmax')(x1)\n","    \n","    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n","    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n","    x2 = tf.keras.layers.LeakyReLU()(x2)\n","    x2 = tf.keras.layers.Dense(1)(x2)\n","    x2 = tf.keras.layers.Flatten()(x2)\n","    x2 = tf.keras.layers.Activation('softmax')(x2)\n","\n","    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n","    model.compile(loss=loss_fn, optimizer=optimizer)\n","    \n","    # this is required as `model.predict` needs a fixed size!\n","    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n","    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n","    \n","    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n","    return model, padded_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijyIkPoF3LJR","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"2642f3c1-6ab4-48b0-9ded-df076bcfc98b"},"source":["DISPLAY=1 # USE display=1 FOR INTERACTIVE\n","preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n","\n","for fold in range(0,5):\n","  \n","  K.clear_session()\n","  model, padded_model = build_model()\n","  path = '/content/gdrive/My Drive/SA/R_CNN_weights/'\n","  weight_fn = path+'v0-roberta-%i.h5'%(fold)\n","\n","  print('Loading model...')\n","  # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n","  load_weights(model, weight_fn) \n","  \n","  print('Predicting Test...')\n","\n","  preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n","  preds_start += preds[0]/num_splits\n","  preds_end += preds[1]/num_splits"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading model...\n","Predicting Test...\n","79/79 [==============================] - 10s 131ms/step\n","Loading model...\n","Predicting Test...\n","79/79 [==============================] - 10s 128ms/step\n","Loading model...\n","Predicting Test...\n","79/79 [==============================] - 10s 129ms/step\n","Loading model...\n","Predicting Test...\n","79/79 [==============================] - 10s 128ms/step\n","Loading model...\n","Predicting Test...\n","79/79 [==============================] - 10s 127ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"65eQNXMJhTwu"},"source":["all = []\n","for k in range(input_ids_t.shape[0]):\n","    a = np.argmax(preds_start[k,])\n","    b = np.argmax(preds_end[k,])\n","    if a>b: \n","        st = test.loc[k,'text']\n","    else:\n","        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n","        enc = tokenizer.encode(text1)\n","        st = tokenizer.decode(enc.ids[a-2:b-1])\n","    all.append(st)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lICoKLSOoHXZ","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"1412c19a-bedd-4367-c784-68e1ebf17131"},"source":["test['selected_text'] = all\n","test.to_csv('/content/gdrive/My Drive/SA/Real-time-Data/Analysis_Data.csv',index=False)\n","pd.set_option('max_colwidth', 60)\n","test.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>retweetcount</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>selected_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>✅ ACC ✅\\n\\nAcc can give Good Breakdown in coming Trading...</td>\n","      <td>positive</td>\n","      <td>good</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>So Pakistan just admitted what India has been saying for...</td>\n","      <td>neutral</td>\n","      <td>so pakistan just admitted what india has been saying fo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>The nationwide lockdown in India which was to end 3 May ...</td>\n","      <td>negative</td>\n","      <td>lockdown</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>COVID 19 cases in India until  250620 2030\\n\\nTotal Case...</td>\n","      <td>positive</td>\n","      <td>recovered 277765 6077 deaths 15042 135🙏🏼</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>A Watch Worth 385 CroresFacts About 5M Luxury Random Fac...</td>\n","      <td>positive</td>\n","      <td>a watch worth 385 croresfacts about 5m luxury random fa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   retweetcount  ...                                                selected_text\n","0             0  ...                                                         good\n","1             0  ...   so pakistan just admitted what india has been saying fo...\n","2             0  ...                                                     lockdown\n","3             0  ...                     recovered 277765 6077 deaths 15042 135🙏🏼\n","4             2  ...   a watch worth 385 croresfacts about 5m luxury random fa...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":12}]}]}